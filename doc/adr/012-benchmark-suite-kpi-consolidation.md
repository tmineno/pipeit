# ADR-012: Benchmark Suite Consolidation and KPI-Driven Optimization Workflow

## Context

The benchmark environment had grown into multiple scripts, legacy suites, and optional telemetry/schema paths that were no longer aligned with the current language/runtime priorities.

Problems observed:

1. Execution/report flow was fragmented across multiple scripts (`run`, `report`, `json->md`) with duplicated logic.
2. Several benchmark items were low-signal for current bottlenecks and did not map cleanly to `pipit-lang-spec-v0.2.x.md` behavior (`tick_rate`, `timer_spin`, overrun, SDF scheduling).
3. Legacy canonical schema + telemetry/perf helpers increased maintenance cost and review complexity for little day-to-day value.
4. Compiler benchmark output lacked explicit KPI grouping comparable to runtime suites.

The project needed a smaller, stable, KPI-oriented benchmark workflow that is easy to run locally and easy to compare over time.

## Decision

Consolidate benchmark operations around a single script and a reduced KPI-aligned suite.

### 1. Single benchmark entrypoint

Use `benches/run_all.sh` as the only benchmark script.

Supported minimal functions:

- Filtered benchmark execution (`--filter`)
- Markdown report generation (`--report`)
- JSON-to-Markdown conversion (`--report --json <path>`)

Supported report behavior:

- `./run_all.sh --report`
- `./run_all.sh --report --output-dir <path>`
- `./run_all.sh --report --json <path> --output-dir <path>`
- `./run_all.sh --report --json <path>`

### 2. Finalized benchmark scope

Keep only these categories as first-class suites:

- `compiler`
- `ringbuf`
- `timer`
- `thread`
- `pdl`

Remove deprecated/unused benchmark executables and helper scripts not required by the minimal workflow.

### 3. KPI-first benchmark definitions

Standardize KPIs across runtime and compiler benches.

- Runtime KPIs:
  - timer jitter/overrun and spin behavior
  - K-factor/tick-rate batching effectiveness
  - task deadline miss rate and scaling
  - ring buffer throughput + contention/backpressure
  - end-to-end PDL task stats
- Compiler KPIs (`compiler/benches/compiler_bench.rs`):
  - parse latency
  - full compile latency (parse -> resolve -> graph -> analyze -> schedule -> codegen)
  - phase latency breakdown
  - parse scaling by task count

### 4. Remove obsolete telemetry/schema path

Delete unused canonical schema and legacy telemetry/report plumbing, and keep reporting purely data-driven from benchmark JSON plus text outputs generated by the active suites.

## Consequences

- Benchmark operations are simpler: one script, one report path, one argument model.
- KPI coverage is clearer and directly tied to current spec/runtime behavior.
- Maintenance burden drops because legacy/unused benchmark infrastructure is removed.
- Historical comparability with removed legacy suites is reduced; future comparisons should be based on the new KPI set.
- `compiler` report integration still depends on text output parsing conventions (non-JSON), so strict machine aggregation remains runtime-JSON-first.

## Alternatives

- **Keep multi-script architecture**: rejected due to duplicated option parsing and drift between execution/report behavior.
- **Retain all legacy suites and telemetry scripts**: rejected due to high maintenance cost and weak KPI relevance.
- **Split compiler and runtime runners**: rejected because user workflow priority is one command for all benchmark actions.

## Exit criteria

- [x] `benches/run_all.sh` is the single benchmark script for run/report/json->md workflows.
- [x] Report CLI behavior matches the four supported invocations above.
- [x] Benchmark categories are reduced to `compiler`, `ringbuf`, `timer`, `thread`, `pdl`.
- [x] Compiler bench is organized by explicit KPI groups.
- [x] Obsolete schema/telemetry helpers are removed from the benchmark workflow.
- [x] `benches/README.md` documents the consolidated behavior and KPI mapping.
